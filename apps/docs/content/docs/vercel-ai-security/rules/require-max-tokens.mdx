---
title: "require-max-tokens"
description: Ensures all AI calls have token limits to prevent resource exhaustion.
category: security
severity: medium
tags: ['security', 'ai', 'llm']
autofix: false
---

> Ensures all AI calls have token limits to prevent resource exhaustion.

## üìä Rule Details

| Property           | Value                                                                                                       |
| ------------------ | ----------------------------------------------------------------------------------------------------------- |
| **Type**           | suggestion                                                                                                  |
| **Severity**       | üü° HIGH                                                                                                     |
| **OWASP LLM**      | [LLM10: Unbounded Consumption](https://owasp.org/www-project-top-10-for-large-language-model-applications/) |
| **CWE**            | [CWE-770: Allocation of Resources Without Limits](https://cwe.mitre.org/data/definitions/770.html)          |
| **CVSS**           | 6.5                                                                                                         |
| **Config Default** | `warn` (recommended), `error` (strict)                                                                      |

## üîç What This Rule Detects

This rule identifies AI SDK calls that don't specify a `maxTokens` limit. Without limits, AI responses can consume excessive tokens, leading to high costs and potential denial of service.

## ‚ùå Incorrect Code

```typescript
// No token limit
await generateText({
  model: openai('gpt-4'),
  prompt: 'Write a story',
});

// Missing maxTokens in stream
await streamText({
  model: anthropic('claude-3'),
  prompt: 'Explain quantum physics',
});
```

## ‚úÖ Correct Code

```typescript
// With token limit
await generateText({
  model: openai('gpt-4'),
  prompt: 'Write a story',
  maxTokens: 4096,
});

// Streaming with limit
await streamText({
  model: anthropic('claude-3'),
  prompt: 'Explain quantum physics',
  maxTokens: 2048,
});
```

## ‚öôÔ∏è Options

| Option             | Type       | Default     | Description                            |
| ------------------ | ---------- | ----------- | -------------------------------------- |
| `allowedFunctions` | `string[]` | `[]`        | Functions that don't require maxTokens |
| `maxRecommended`   | `number`   | `undefined` | Warn if maxTokens exceeds this value   |

## üõ°Ô∏è Why This Matters

Unbounded token consumption can cause:

- **Cost explosion** - Each token costs money
- **Denial of service** - API rate limits exhausted
- **Slow responses** - Long generations impact UX
- **Resource starvation** - Other requests may be blocked

## üîó Related Rules

- [`require-max-steps`](./require-max-steps.md) - Limit multi-step tool calling
- [`require-abort-signal`](./require-abort-signal.md) - Enable cancellation

## Known False Negatives

The following patterns are **not detected** due to static analysis limitations:

### Options from Variable

**Why**: Options stored in variables are not analyzed.

```typescript
// ‚ùå NOT DETECTED - Options from variable
const options = { model: openai('gpt-4'), prompt: 'Hello' }; // Missing maxTokens
await generateText(options);
```

**Mitigation**: Use inline options. Always specify maxTokens explicitly.

### Spread Configuration

**Why**: Spread may hide that maxTokens is missing.

```typescript
// ‚ùå NOT DETECTED - maxTokens may not be in base
const base = getModelConfig();
await generateText({ ...base, prompt: 'Hello' }); // maxTokens?
```

**Mitigation**: Always set maxTokens explicitly. Don't rely on spread configs.

### Wrapper Functions

**Why**: Custom wrapper functions are not recognized.

```typescript
// ‚ùå NOT DETECTED - Wrapper hides missing maxTokens
const result = await myGenerateText('Hello'); // Wrapper may not set limit
```

**Mitigation**: Apply rule to wrapper implementations.

### Model Default Limits

**Why**: Model-specific defaults are not considered.

```typescript
// ‚ö†Ô∏è MAY FLAG - Model has reasonable default
await generateText({
  model: openai('gpt-4-turbo'), // Has 4096 default
  prompt: 'Hello',
});
```

**Mitigation**: Explicitly set maxTokens for clarity.

## üìö References

- [OWASP LLM10: Unbounded Consumption](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [CWE-770: Allocation of Resources Without Limits](https://cwe.mitre.org/data/definitions/770.html)
- [Vercel AI SDK Generation Options](https://sdk.vercel.ai/docs/ai-sdk-core/generating-text)
