---
title: "no-sensitive-in-prompt"
description: Prevents sensitive data (passwords, tokens, PII) from being sent to LLMs.
category: security
severity: medium
tags: ['security', 'ai', 'llm']
autofix: false
---

> Prevents sensitive data (passwords, tokens, PII) from being sent to LLMs.

## üìä Rule Details

| Property           | Value                                                                                                                  |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------- |
| **Type**           | problem                                                                                                                |
| **Severity**       | üî¥ CRITICAL                                                                                                            |
| **OWASP LLM**      | [LLM02: Sensitive Information Disclosure](https://owasp.org/www-project-top-10-for-large-language-model-applications/) |
| **CWE**            | [CWE-200: Information Exposure](https://cwe.mitre.org/data/definitions/200.html)                                       |
| **CVSS**           | 8.5                                                                                                                    |
| **Config Default** | `error` (recommended, strict)                                                                                          |

## üîç What This Rule Detects

This rule identifies code patterns where sensitive data like passwords, API keys, tokens, or personally identifiable information (PII) is passed to AI prompts. LLM providers may log, store, or use this data for training.

## ‚ùå Incorrect Code

```typescript
// Password in prompt
await generateText({
  prompt: `Reset password for user. Current password: ${userPassword}`,
});

// API key in prompt
await generateText({
  prompt: `Configure service with key: ${apiKey}`,
});

// SSN in prompt
await streamText({
  prompt: `Process application for SSN: ${socialSecurityNumber}`,
});

// Credit card in prompt
await generateText({
  prompt: `Validate credit card: ${creditCardNumber}`,
});
```

## ‚úÖ Correct Code

```typescript
// Redacted data
await generateText({
  prompt: `Reset password for user. Current password: [REDACTED]`,
});

// No sensitive data
await generateText({
  prompt: `Configure service with the API key stored in environment variables.`,
});

// Use redaction helper
await streamText({
  prompt: `Process application for SSN: ${redact(socialSecurityNumber)}`,
});

// Reference instead of value
await generateText({
  prompt: `Validate the credit card on file for user ${userId}`,
});
```

## ‚öôÔ∏è Options

| Option              | Type       | Default                                                                                                                          | Description                                 |
| ------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| `sensitivePatterns` | `string[]` | `['password', 'secret', 'token', 'apiKey', 'api_key', 'credential', 'ssn', 'socialSecurity', 'creditCard', 'cvv', 'privateKey']` | Variable patterns suggesting sensitive data |

## üõ°Ô∏è Why This Matters

Sending sensitive data to LLMs can result in:

- **Data breach** - LLM providers may store prompts
- **Training data poisoning** - Your data may be used to train models
- **Compliance violations** - GDPR, HIPAA, PCI-DSS violations
- **Third-party exposure** - Data shared with third-party AI providers

## üîó Related Rules

- [`no-hardcoded-api-keys`](./no-hardcoded-api-keys.md) - Prevent hardcoded credentials
- [`require-output-filtering`](./require-output-filtering.md) - Filter sensitive tool output

## Known False Negatives

The following patterns are **not detected** due to static analysis limitations:

### Variable Names Not Matching Patterns

**Why**: Only configured sensitive patterns are checked.

```typescript
// ‚ùå NOT DETECTED - Custom field name
await generateText({
  prompt: `Process data: ${mySecretField}`, // Not in sensitivePatterns
});
```

**Mitigation**: Configure `sensitivePatterns` with custom field names.

### Dynamic Prompt Construction

**Why**: Prompts built at runtime are not analyzed.

```typescript
// ‚ùå NOT DETECTED - Dynamic prompt
const fields = [userPassword, apiKey];
const prompt = fields.join(', ');
await generateText({ prompt });
```

**Mitigation**: Never concatenate sensitive data into prompts.

### Nested Object Properties

**Why**: Deep property access may not be recognized.

```typescript
// ‚ùå NOT DETECTED - Nested property
await generateText({
  prompt: `Reset with: ${user.credentials.password}`,
});
```

**Mitigation**: Configure patterns to match nested sensitive fields.

### Encrypted/Transformed Data

**Why**: Transformed data appears safe but may be sensitive.

```typescript
// ‚ùå NOT DETECTED - Encrypted but still sensitive
await generateText({
  prompt: `Decrypt this: ${encryptedPassword}`,
});
```

**Mitigation**: Never send any form of credentials to LLMs.

## üìö References

- [OWASP LLM02: Sensitive Information Disclosure](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [CWE-200: Information Exposure](https://cwe.mitre.org/data/definitions/200.html)
